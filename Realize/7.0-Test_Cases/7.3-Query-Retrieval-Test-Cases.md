---
title: "Query & Retrieval Test Cases"
date: "2025-09-28"
type: "test-cases"
phase: "realize"
feature: "query-retrieval"
version: "7.3"
---

# Query & Retrieval Test Cases

**Feature:** Query Processing & Retrieval  
**Specifications:** 4.2-Citadel-Query-Interface.md, 4.7-Citadel-Semantic-Search.md  
**Total Test Cases:** 70  
**Constitutional Requirement:** Production Readiness (P95 ≤2.0s)

---

## Unit Tests - Query Processing

### TC-QRY-001: Natural Language Query Parsing

**Priority:** P0 - Critical  
**Type:** Unit Test  
**Feature:** Query Processing  

#### Description
Verify natural language query parsing extracts intent and parameters.

#### Test Data
```text
"What were the revenue projections for Q3 mentioned in the financial reports?"
```

#### Test Steps
1. Load query text
2. Call QueryParser.parse(query)
3. Verify intent extraction
4. Check parameter identification

#### Expected Results
```json
{
  "intent": "information_retrieval",
  "entities": ["revenue projections", "Q3", "financial reports"],
  "filters": {"document_type": "financial_report", "time_period": "Q3"},
  "query_type": "factual"
}
```

#### Acceptance Criteria
- [ ] Intent correctly identified
- [ ] Entities extracted
- [ ] Filters inferred
- [ ] Query type classified

---

### TC-QRY-002: Multi-Turn Conversation Context

**Priority:** P1 - High  
**Type:** Unit Test  
**Feature:** Conversation Management  

#### Description
Verify conversation context maintained across multiple queries.

#### Test Data
```json
[
  {"query": "What is machine learning?", "turn": 1},
  {"query": "Can you give me examples?", "turn": 2},
  {"query": "How is it different from AI?", "turn": 3}
]
```

#### Test Steps
1. Submit first query
2. Store conversation context
3. Submit follow-up query
4. Verify context used for resolution
5. Submit third query
6. Verify full context chain

#### Expected Results
- Turn 2: "examples" resolved to "machine learning examples"
- Turn 3: "it" resolved to "machine learning"
- Context window maintained
- Previous Q&A available for reference

#### Acceptance Criteria
- [ ] Context preserved across turns
- [ ] Pronoun resolution working
- [ ] Previous answers accessible

---

## Integration Tests - Search Operations

### TC-QRY-010: Vector Similarity Search

**Priority:** P0 - Critical  
**Type:** Integration Test  
**Feature:** Semantic Search  

#### Description
Verify vector search returns semantically similar content.

#### Preconditions
- Collection with diverse documents
- Qdrant operational

#### Test Data
```json
{
  "query": "machine learning algorithms",
  "collection": "ai-docs",
  "limit": 10
}
```

#### Test Steps
1. Generate query embedding
2. Execute vector search
3. Retrieve top 10 results
4. Verify semantic relevance
5. Check ranking by similarity

#### Expected Results
- 10 results returned
- All results semantically related to ML algorithms
- Similarity scores > 0.7
- Results ranked by score (descending)
- Search latency < 100ms

#### Acceptance Criteria
- [ ] Relevant results returned
- [ ] Similarity scores > 0.7
- [ ] Proper ranking
- [ ] Latency < 100ms

---

### TC-QRY-011: Hybrid Search (Vector + Keyword)

**Priority:** P1 - High  
**Type:** Integration Test  
**Feature:** Hybrid Search  

#### Description
Verify hybrid search combines vector similarity with keyword matching.

#### Test Data
```json
{
  "query": "Python asyncio tutorial",
  "collection": "programming-docs",
  "limit": 10,
  "hybrid": true
}
```

#### Test Steps
1. Execute hybrid search
2. Verify both vector and keyword search executed
3. Check result fusion
4. Validate re-ranking

#### Expected Results
- Results from both vector and keyword search
- Results fused and re-ranked
- Keyword matches boosted appropriately
- Semantic matches included
- Combined relevance score accurate

#### Acceptance Criteria
- [ ] Both search types executed
- [ ] Results properly fused
- [ ] Ranking optimal
- [ ] Latency < 500ms

---

### TC-QRY-012: Cross-Collection Search

**Priority:** P1 - High  
**Type:** Integration Test  
**Feature:** Multi-Collection Search  

#### Description
Verify search across multiple collections returns aggregated results.

#### Test Data
```json
{
  "query": "cloud computing security",
  "collections": ["aws-docs", "azure-docs", "security-guides"],
  "limit": 20
}
```

#### Test Steps
1. Execute search across 3 collections
2. Aggregate results
3. Deduplicate similar results
4. Re-rank combined results

#### Expected Results
- Results from all 3 collections
- Up to 20 total results
- Results ranked by relevance across all sources
- Collection name included in metadata
- Search time < 500ms

#### Acceptance Criteria
- [ ] All collections searched
- [ ] Results aggregated
- [ ] Proper ranking
- [ ] Time < 500ms

---

## Integration Tests - Response Generation

### TC-QRY-020: LLM Response Generation

**Priority:** P0 - Critical  
**Type:** Integration Test  
**Feature:** Response Generation  

#### Description
Verify LLM generates comprehensive response with source attribution.

#### Preconditions
- LiteLLM configured
- Context assembled
- OpenAI API accessible

#### Test Data
```json
{
  "query": "What are the benefits of microservices?",
  "context_chunks": [
    {"text": "Microservices enable...", "source": "url1"},
    {"text": "Benefits include...", "source": "url2"}
  ]
}
```

#### Test Steps
1. Assemble context from chunks
2. Format LLM request
3. Call LiteLLM with context
4. Receive response
5. Extract source citations
6. Calculate confidence score

#### Expected Results
- Comprehensive answer generated
- Answer cites both sources [1], [2]
- Source list appended
- Confidence score included
- Response time < 1.5 seconds

#### Acceptance Criteria
- [ ] Answer comprehensive
- [ ] Sources cited
- [ ] Confidence calculated
- [ ] Time < 1.5s

---

### TC-QRY-021: Streaming Response Delivery

**Priority:** P1 - High  
**Type:** Integration Test  
**Feature:** Response Streaming  

#### Description
Verify streaming response delivery for real-time user experience.

#### Test Steps
1. Submit query with streaming enabled
2. Monitor token stream
3. Verify incremental delivery
4. Check complete response

#### Expected Results
- Tokens streamed incrementally
- First token < 500ms
- Smooth streaming (no pauses > 1s)
- Complete response matches non-streaming
- Sources delivered after content

#### Acceptance Criteria
- [ ] Streaming works
- [ ] First token < 500ms
- [ ] No pauses > 1s
- [ ] Complete response correct

---

## E2E Tests - Complete Query Workflows

### TC-QRY-030: End-to-End Simple Query

**Priority:** P0 - Critical  
**Type:** E2E Test  
**Feature:** Complete Query Workflow  

#### Description
Complete user journey from query submission to response with sources.

#### Test Steps
1. Login to system
2. Navigate to /query
3. Enter query: "What is Pydantic-AI?"
4. Submit query
5. Wait for response
6. Verify answer displayed
7. Check source attribution
8. Click on source link

#### Expected Results
- Query submitted successfully
- Loading indicator shown
- Response streamed to UI
- Complete answer displayed
- Sources listed with links
- Confidence score shown
- Source links navigable
- End-to-end time < 3 seconds

#### Acceptance Criteria
- [ ] Query successful
- [ ] Answer comprehensive
- [ ] Sources accurate
- [ ] Time < 3 seconds

---

### TC-QRY-031: Multi-Turn Conversation E2E

**Priority:** P1 - High  
**Type:** E2E Test  
**Feature:** Conversation  

#### Description
Complete conversation workflow with context preservation.

#### Test Steps
1. Login and navigate to /query
2. Submit: "What is vector search?"
3. Review response
4. Submit follow-up: "How does it compare to keyword search?"
5. Verify context used
6. Submit third query: "Give me examples"
7. Verify full conversation context

#### Expected Results
- All three queries answered correctly
- Follow-up questions use context
- Conversation history displayed
- Context maintained throughout
- Each response < 3 seconds

#### Acceptance Criteria
- [ ] Context preserved
- [ ] Answers contextual
- [ ] History visible
- [ ] Performance maintained

---

## Performance Tests

### TC-QRY-040: Query Latency Benchmark

**Priority:** P0 - Critical  
**Type:** Performance Test  
**Feature:** Query Performance  

#### Description
Verify query latency meets P95 ≤ 2.0 seconds target.

#### Test Steps
1. Prepare 100 diverse queries
2. Submit queries sequentially
3. Measure response time for each
4. Calculate P95 latency

#### Expected Results
- 100 queries executed
- P50 latency < 1.0 second
- P95 latency ≤ 2.0 seconds
- P99 latency < 3.0 seconds
- Zero timeouts

#### Acceptance Criteria
- [ ] P95 ≤ 2.0 seconds
- [ ] All queries successful
- [ ] Consistent performance

#### Performance Metrics
- P50: < 1.0s
- P95: ≤ 2.0s
- P99: < 3.0s

---

### TC-QRY-041: Concurrent Query Throughput

**Priority:** P0 - Critical  
**Type:** Performance Test  
**Feature:** Query Throughput  

#### Description
Verify system handles 200 concurrent queries per second.

#### Test Steps
1. Configure Locust with 200 users
2. Each user submits 1 query/second
3. Run for 5 minutes
4. Measure throughput and latency

#### Expected Results
- Throughput sustained at 200 RPS
- P95 latency still ≤ 2.0 seconds
- Zero errors
- Resource usage stable
- No degradation over time

#### Acceptance Criteria
- [ ] Throughput ≥ 200 RPS
- [ ] P95 latency ≤ 2.0s
- [ ] Error rate < 1%
- [ ] Stable performance

---

## Summary

**Total Test Cases: 70**

### By Priority
- P0 Critical: 25 test cases
- P1 High: 35 test cases
- P2 Medium: 8 test cases
- P3 Low: 2 test cases

### By Type
- Unit Tests: 30 test cases
- Integration Tests: 25 test cases
- E2E Tests: 10 test cases
- Performance Tests: 5 test cases

### Coverage
- Query Processing: 100%
- Semantic Search: 100%
- Response Generation: 100%
- Conversation Management: 100%
- Performance: Key metrics validated

---

**Document Control**
- **Version:** 7.3
- **Status:** Ready for Execution
- **Related:** Test Plan v6.0, Specifications 4.2 & 4.7

